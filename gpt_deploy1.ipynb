{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvzxATAX7F3Td2JqWyhHSZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8e6f71118ff74ba988f3438cbd062739":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01fd55d823454618ab598340df32249c","IPY_MODEL_a8e399e0dcc2448cbc30b1768918eb91","IPY_MODEL_8ac4934e98f743e1a6d88968de11d95a"],"layout":"IPY_MODEL_0a696e1f50554b0297829cc203438238"}},"01fd55d823454618ab598340df32249c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bba34810aebf4f7787c1832d387fcc24","placeholder":"​","style":"IPY_MODEL_2163d252881845b1bb17341299ff3fc0","value":"Downloading: 100%"}},"a8e399e0dcc2448cbc30b1768918eb91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c1a71bf7884c408501a22adf227ebd","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65439fb959cd47dfbe1e39101b171a8a","value":2825034}},"8ac4934e98f743e1a6d88968de11d95a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6788a16356147ef870058ebc00dea7c","placeholder":"​","style":"IPY_MODEL_2e6a7959b07d4e969efad1ed87b5883d","value":" 2.83M/2.83M [00:00&lt;00:00, 12.1MB/s]"}},"0a696e1f50554b0297829cc203438238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bba34810aebf4f7787c1832d387fcc24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2163d252881845b1bb17341299ff3fc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37c1a71bf7884c408501a22adf227ebd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65439fb959cd47dfbe1e39101b171a8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6788a16356147ef870058ebc00dea7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e6a7959b07d4e969efad1ed87b5883d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itwTdBDlcAVQ","executionInfo":{"status":"ok","timestamp":1674004457703,"user_tz":-540,"elapsed":16037,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"77f1b365-2d15-4873-fd87-444bcb38b5e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["!pip install ratsnlp"],"metadata":{"id":"lyHkNBYbcWpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel\n","model = GPT2LMHeadModel.from_pretrained(\n","    'skt/kogpt2-base-v2',\n",")\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"SuArWnEMcXp_","executionInfo":{"status":"error","timestamp":1674110934373,"user_tz":-540,"elapsed":17,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"5e7504b6-1f6a-4621-d61d-1aa6bc06cdca"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-81a25c363523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mratsnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlpbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationDeployArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m args = GenerationDeployArguments(\n\u001b[1;32m      3\u001b[0m     \u001b[0mpretrained_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'skt/kogpt2-base-v2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdownstream_model_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/gdrive/MyDrive/nlpbook/checkpoint-generation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ratsnlp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\n","    'skt/kogpt2-base-v2',\n","    eos_token='</s>'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["8e6f71118ff74ba988f3438cbd062739","01fd55d823454618ab598340df32249c","a8e399e0dcc2448cbc30b1768918eb91","8ac4934e98f743e1a6d88968de11d95a","0a696e1f50554b0297829cc203438238","bba34810aebf4f7787c1832d387fcc24","2163d252881845b1bb17341299ff3fc0","37c1a71bf7884c408501a22adf227ebd","65439fb959cd47dfbe1e39101b171a8a","d6788a16356147ef870058ebc00dea7c","2e6a7959b07d4e969efad1ed87b5883d"]},"id":"Qv0BcKnbcfhU","executionInfo":{"status":"ok","timestamp":1674004692166,"user_tz":-540,"elapsed":1203,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"997c14d6-b6d4-4998-adad-3295e5cd5e1d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6f71118ff74ba988f3438cbd062739"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]}]},{"cell_type":"markdown","source":["프롬프트 준비"],"metadata":{"id":"yg3OmRj2dL06"}},{"cell_type":"code","source":["input_ids = tokenizer.encode('안녕하세요', return_tensors='pt')\n","input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFWOGbuXdHQj","executionInfo":{"status":"ok","timestamp":1674004742698,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"763cfc0c-615f-4554-f25c-1139d4879a95"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[25906,  8702,  7801,  8084]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["- 그리디 서치(greedy search)는 매순간 최선(best)를 선택해 탐색 범위를 줄여보자는 것이 핵심 아이디어\n","- 빔 서치(beam search)는 빔(beam) 크기만큼의 선택지를 계산 범위에 포함\n","- 탑k 샘플링(top-k sampling)은 \n","  - 모델이 예측한 다음 토큰 확률 분포 에서 확률값이 가장 높은  k 개 토큰 가운데 하나를 다음 토큰으로 선택하는 기법. \n","  - k개 안에 있는 단어라면 의자 같이 확률값이 낮은 케이스도 다음 토큰으로 추출될 수 있습니다. 따라서 탑k 샘플링은 매 시행 때마다 생성 결과가 달라집니다.\n","  - top_k를 1로 입력한다면 do_sample 인자를 True로 두더라도 그리디 서치와 동일한 효과\n","- 템퍼러처 스케일링(temperature scaling)이란 \n","  - 모델의 다음 토큰 확률분포에 변형을 가해 문장을 다양하게 생성하는 기법\n","  - 확률분포를 변형한다는 의미는, 대소 관계의 역전 없이 분포의 모양만을 바꾼다는 의미\n","  - 이 값이 0에 가까울 수록 확률분포 모양이 원래 대비 뾰족해 진다. 순위의 변동은 없지만 원래 컸던 확률은 더 커지고, 작았던 확률은 더 작아져 확률분포의 모양이 뾰족(sharp)해진다. 그만큼 확률값 기준 1등 토큰이 다음 토큰으로 뽑힐 가능성이 높아진다\n","  - temperature를 1보다 작게 하면 상대적으로 정확한 문장을, 1보다 크게 하면 상대적으로 다양한 문장을 생성한다.\n","  - 템퍼러처 스케일링은 탑k 샘플링, 탑p 샘플링과 같이 적용해야 의미가 있다.\n","- 탑p 샘플링(top-p sampling)은 \n","  - 확률값이 높은 순서대로 내림차순 정렬을 한 뒤 누적 확률값이  p 이하인 단어들 가운데 하나를 다음 단어로 선택하는 기법\n","  - 뉴클리어스 샘플링(necleus sampling)이라고도 불리며 확률값을 기준으로 단어들을 내림차순 정렬해 그 값이 높은 단어들을 후보로 삼는다는 점에서는 탑k 샘플링과 같지만 상위  k개를 후보로 삼느냐(탑k 샘플링), 누적 확률값이  p\n","  이하인 단어들을 후보로 삼느냐(탑p 샘플링)에 따라 차이\n","- 리피티션 패널티(repetition penalty)라는 방식으로 반복을 통제할 수도 있습니다. repetition_penalty라는 인자를 주면 됩니다. 그 값은 1.0 이상이어야 하며 클 수록 페널티가 세게 적용"],"metadata":{"id":"YK_gSmeXdtCa"}},{"cell_type":"markdown","source":["Greedy Search\n","\n","다음 단어 확률 분포에서 최대 확률을 내는 단어들을 리턴합니다. \n","여러 번 수행하더라도 생성 결과가 바뀌지 않습니다 (`do_sample=False`).\n","`max_length`는 생성 최대 길이이며 이보다 길거나, 짧더라도 EOD 토큰 등 스페셜 토큰이 나타나면 생성을 중단합니다. `min_length`는 생성 최소 길이이며 이보다 짧은 구간에서 EOD 등 스페셜 토큰이 등장해 생성이 중단될 경우 해당 토큰이 나올 확률을 0으로 수정하여 문장 생성이 종료되지 않도록 강제합니다"],"metadata":{"id":"mQUjzKswhQxK"}},{"cell_type":"code","source":["import torch\n","with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample=False,\n","      min_length=10,\n","      max_length=50\n","  )"],"metadata":{"id":"NlSrF3D0dRu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2taMIqdhe-t","executionInfo":{"status":"ok","timestamp":1674005891190,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"ec327964-d7a4-4cca-d20f-12f3f9204588"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"그럼, 그건 뭐예요?\"\n","\n"]}]},{"cell_type":"markdown","source":["Beam Search \n","\n","Beam Search는 다음 단어 확률 분포에서 `num_beams`만큼의 경우의 수를 남겨가면서 문장을 생성합니다. Beam search는 Greedy search보다 계산량이 많지만 좀 더 확률값이 높은 문장을 생성할 수 있습니다."],"metadata":{"id":"FcLSLZRBiPpQ"}},{"cell_type":"code","source":["with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample=False,\n","      min_length=10,\n","      max_length=50,\n","      num_beams=3\n","  )"],"metadata":{"id":"PlQjohHRhsFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcRY3fDNidrr","executionInfo":{"status":"ok","timestamp":1674006120331,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"9f77291f-d712-4fe0-c43f-c28c8613beea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요?\"\n","\"그렇지 않습니다.\"\n","\"그렇지 않습니다.\"\n","\"그렇지 않습니다.\"\n","\"그렇지 않습니다.\"\n","\"그렇지 않습니다.\"\n","\"그렇지 않습니다.\"\n","\"그\n"]}]},{"cell_type":"markdown","source":["반복 줄이기\n","\n","반복되는 n-gram 사이즈를 지정하기\n","\n","위의 예시를 보면 `\"그럼, 그건 뭐예요?\"`이 반복됩니다. 이를 아래와 같이 지정해 반복을 방지합니다. 3개 이상의 토큰이 반복될 경우 해당 3-gram 등장 확률을 0으로 만들어 생성 결과에서 배제합니다."],"metadata":{"id":"5CQqNuHhjspj"}},{"cell_type":"code","source":["with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample=False,\n","      min_length=10,\n","      max_length=50,\n","      no_repeat_ngram_size=5 #토큰 3개 이상 반보시 3번째 토큰 확률 0으로 변경\n","  )"],"metadata":{"id":"6R2Br13EikNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YA8hEV0FkDdD","executionInfo":{"status":"ok","timestamp":1674006538275,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"0041f5c2-d036-4685-b4b4-8d36cae1623e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"아니요, 그건 뭐예요.\"\n","\"그럼요, 그건 뭐죠?\"\n","\"그건 뭐예요!\"\n","\"그건 무슨 말씀\n"]}]},{"cell_type":"markdown","source":["repetition penalty\n","\n","repetition penalty로 반복을 통제할 수도 있습니다. 다음과 같이 실행하면 되며 그 범위는 1 이상의 값을 가져야 합니다. 1이라면 아무런 패널티를 적용하지 않는게 됩니다."],"metadata":{"id":"n_Aod0hekaTs"}},{"cell_type":"code","source":["with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample=False,\n","      min_length=10,\n","      max_length=50,\n","      repetition_penalty = 1.5\n","  )"],"metadata":{"id":"udljtBjzkHdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgYGeOg0kiub","executionInfo":{"status":"ok","timestamp":1674006665670,"user_tz":-540,"elapsed":23,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"a2d4fa2a-9941-408d-ff48-faf2f2f61988"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요?\"\n","\"그럼, 그건 뭐예요, 아저씨. 저는 지금 이 순간에도 괜찮아요. 그리고 제가 할 수 있는 일은 아무것도 없어요.\n","이제 그만 돌아가고 싶어요.\n","제가 하는 일이 무엇\n"]}]},{"cell_type":"markdown","source":["top-k sampling\n","\n","지금까지는 생성을 반복하더라도 그 결과가 동일한 샘플링 방식을 살펴봤습니다. top-k sampling은 다음 단어를 뽑을 때 확률값 기준 가장 큰 k개 가운데 하나를 선택하는 기법입니다. 확률값이 큰 단어가 다음 단어로 뽑힐 가능성이 높아지지만, k개 안에 있는 단어라면 확률값이 낮더라도 다음 단어로 추출될 수 있습니다. 따라서 top-k sampling은 매 시행 때마다 생성 결과가 달라집니다. k는 1 이상의 값을 지녀야 합니다."],"metadata":{"id":"TmbVrFV9oJHP"}},{"cell_type":"code","source":["with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample=True,\n","      min_length=10,\n","      max_length=50,\n","      top_k = 50 #확률값이 가장 높은 k개 토큰 가운데 하나를 다음 토큰으로 선택\n","  )"],"metadata":{"id":"L-hpHWrskl9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5UcCZR3oXB0","executionInfo":{"status":"ok","timestamp":1674007652041,"user_tz":-540,"elapsed":300,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"e00e431d-a05b-4633-8f21-1e10554ed3cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요'를, \"가족을 위해 사랑을 실천하는 것도 중요하지만 그걸 지켜 나가는 건 나 하나만큼은 해야 한다\"는 등의 내용이 있다.\n","실제 한 누리꾼은 \"결혼을 준비 중인 부부가 신혼여행을 가든 다른 사람이 결혼\n"]}]},{"cell_type":"markdown","source":["top-k sampling + temperature scaling\n","\n","top-k sampling은 temperature scaling과 동시에 적용할 수 있습니다. 그 값에 따라 다음과 같은 효과가 납니다.\n","\n","t가 0에 가까워질 수록 토큰 분포가 sharp해진다 > 1등 토큰이 뽑힐 확률이 그만큼 높아진다 > do_sample=True이지만 사실상 greedy decoding이 된다"],"metadata":{"id":"RIqfznhqpDMw"}},{"cell_type":"code","source":["with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample = True,\n","      min_length = 10,\n","      max_length = 50,\n","      top_k = 50,\n","      temperature = 0.01\n","  )"],"metadata":{"id":"RGE4EOnAoaDA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTtGtPChpPPB","executionInfo":{"status":"ok","timestamp":1674007879902,"user_tz":-540,"elapsed":291,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"323a1412-7a3a-456f-e872-8d2026ac12d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"그럼, 그건 뭐예요?\"\n","\"그럼, 그건 뭐예요?\"\n","\n"]}]},{"cell_type":"markdown","source":["top-p sampling\n","\n","top-p sampling은 다음 단어를 뽑을 때 누적 확률값이 p 이하인 단어들 가운데 하나를 선택하는 기법입니다. 확률값이 큰 단어가 다음 단어로 뽑힐 가능성이 높아지지만, 누적 확률값 p 이하에 있는 단어라면 확률값이 낮더라도 다음 단어로 추출될 수 있습니다. 따라서 top-p sampling은 매 시행 때마다 생성 결과가 달라집니다. p는 확률이기 때문에 0~1 사이의 값을 지녀야 합니다. p가 1이라면 어휘 집합에 있는 모든 단어를 대상으로 샘플링하기 때문에 top-p sampling 효과가 사라집니다. p가 1보다 약간 작다면 확률값이 낮은 일부 단어들을 다음 단어 후보에서 제거해 생성 품질을 높입니다"],"metadata":{"id":"2p--9K83q4jk"}},{"cell_type":"code","source":["with torch.no_grad():\n","  generated_ids = model.generate(\n","      input_ids,\n","      do_sample = True,\n","      min_length = 10,\n","      max_length = 50,\n","      top_p = 0.92,\n","  )"],"metadata":{"id":"uFPTdShypRrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([el.item() for el in generated_ids[0]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xopwOunfrBDK","executionInfo":{"status":"ok","timestamp":1674008367444,"user_tz":-540,"elapsed":35,"user":{"displayName":"Jihun Park","userId":"16714741662004354318"}},"outputId":"5492f8d4-5efb-4c6b-87a4-c84bb4ae0727"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕하세요”라고 인사를 건넸고 그는 또 “박근혜 대통령 취임 당시 한진해운을 살리겠다고 했다”고 강조했다.\n","당시 박 대통령은 한진해운과 아시아나가 합쳐지면 한국 경제의 성장엔진은 사라질 것이라며 “정부가\n"]}]}]}